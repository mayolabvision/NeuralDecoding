{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 00:13:32.801390: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from handy_functions.preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from handy_functions.metrics import get_R2\n",
    "from handy_functions.metrics import get_rho\n",
    "\n",
    "#Import decoder functions\n",
    "from handy_functions.decoders import WienerCascadeDecoder\n",
    "from handy_functions.decoders import WienerFilterDecoder\n",
    "from handy_functions.decoders import DenseNNDecoder\n",
    "from handy_functions.decoders import SimpleRNNDecoder\n",
    "from handy_functions.decoders import GRUDecoder\n",
    "from handy_functions.decoders import LSTMDecoder\n",
    "from handy_functions.decoders import XGBoostDecoder\n",
    "from handy_functions.decoders import SVRDecoder\n",
    "\n",
    "\n",
    "from sklearn import linear_model #For Wiener Filter and Wiener Cascade\n",
    "from sklearn.svm import SVR #For support vector regression (SVR)\n",
    "from sklearn.svm import SVC #For support vector classification (SVM)\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "folder = '/Users/kendranoneman/Projects/mayo/NeuralDecoding/datasets/'\n",
    "\n",
    "with open(folder+'MTFEF-pa29dir4A-1600ms.pickle','rb') as f:\n",
    "    neural_data,vels_binned=pickle.load(f,encoding='latin1') #If using python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71647, 65)\n",
      "(71647, 2)\n"
     ]
    }
   ],
   "source": [
    "print(neural_data.shape)\n",
    "num_timebins  =  neural_data.shape[0]\n",
    "num_neurons   =  neural_data.shape[1]\n",
    "print(vels_binned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71647, 13, 65)\n",
      "(71647, 845)\n",
      "(71647, 2)\n"
     ]
    }
   ],
   "source": [
    "bins_before = 6 #How many bins of neural data prior to the output are used for decoding\n",
    "bins_current = 1 #Whether to use concurrent time bin of neural data\n",
    "bins_after = 6 #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "# Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "# Function to get the covariate matrix that includes spike history from previous bins\n",
    "X = get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "print(X.shape)\n",
    "\n",
    "# Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "#Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "print(X_flat.shape)\n",
    "\n",
    "y=vels_binned\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_range=[0, 0.7]\n",
    "testing_range=[0.7, 0.85]\n",
    "valid_range=[0.85,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/jngvvwrj1p722fqyx4vypp540000gn/T/ipykernel_58127/2072859609.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  training_set=np.arange(np.int(np.round(training_range[0]*num_examples))+bins_before,np.int(np.round(training_range[1]*num_examples))-bins_after)\n",
      "/var/folders/gf/jngvvwrj1p722fqyx4vypp540000gn/T/ipykernel_58127/2072859609.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  testing_set=np.arange(np.int(np.round(testing_range[0]*num_examples))+bins_before,np.int(np.round(testing_range[1]*num_examples))-bins_after)\n",
      "/var/folders/gf/jngvvwrj1p722fqyx4vypp540000gn/T/ipykernel_58127/2072859609.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  valid_set=np.arange(np.int(np.round(valid_range[0]*num_examples))+bins_before,np.int(np.round(valid_range[1]*num_examples))-bins_after)\n"
     ]
    }
   ],
   "source": [
    "num_examples=X.shape[0]\n",
    "print(num_examples)\n",
    "\n",
    "#Note that each range has a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "#This makes it so that the different sets don't include overlapping neural data\n",
    "training_set=np.arange(np.int(np.round(training_range[0]*num_examples))+bins_before,np.int(np.round(training_range[1]*num_examples))-bins_after)\n",
    "testing_set=np.arange(np.int(np.round(testing_range[0]*num_examples))+bins_before,np.int(np.round(testing_range[1]*num_examples))-bins_after)\n",
    "valid_set=np.arange(np.int(np.round(valid_range[0]*num_examples))+bins_before,np.int(np.round(valid_range[1]*num_examples))-bins_after)\n",
    "\n",
    "#Get training data\n",
    "X_train=X[training_set,:,:]\n",
    "X_flat_train=X_flat[training_set,:]\n",
    "y_train=y[training_set,:]\n",
    "\n",
    "#Get testing data\n",
    "X_test=X[testing_set,:,:]\n",
    "X_flat_test=X_flat[testing_set,:]\n",
    "y_test=y[testing_set,:]\n",
    "\n",
    "#Get validation data\n",
    "X_valid=X[valid_set,:,:]\n",
    "X_flat_valid=X_flat[valid_set,:]\n",
    "y_valid=y[valid_set,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean=np.nanmean(X_train,axis=0)\n",
    "X_train_std=np.nanstd(X_train,axis=0)\n",
    "X_train=(X_train-X_train_mean)/X_train_std\n",
    "X_test=(X_test-X_train_mean)/X_train_std\n",
    "X_valid=(X_valid-X_train_mean)/X_train_std\n",
    "\n",
    "#Z-score \"X_flat\" inputs. \n",
    "X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "X_flat_train_std=np.nanstd(X_flat_train,axis=0)\n",
    "X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "X_flat_valid=(X_flat_valid-X_flat_train_mean)/X_flat_train_std\n",
    "\n",
    "#Zero-center outputs\n",
    "y_train_mean=np.mean(y_train,axis=0)\n",
    "y_train=y_train-y_train_mean\n",
    "y_test=y_test-y_train_mean\n",
    "y_valid=y_valid-y_train_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiener Filter Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2s: [0.27381409 0.52898313]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_wf=WienerFilterDecoder()\n",
    "\n",
    "#Fit model\n",
    "model_wf.fit(X_flat_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_wf=model_wf.predict(X_flat_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_wf=get_R2(y_valid,y_valid_predicted_wf)\n",
    "print('R2s:', R2s_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('results/decoding_data_1600ms_wf_pa29.mat',{'y_valid': y_valid,'y_train_mean': y_train_mean, 'y_valid_predicted_wf': y_valid_predicted_wf,'R2s_wf':R2s_wf})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiener Cascade Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2s: [0.28008279 0.5361358 ]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_wc=WienerCascadeDecoder(degree=3)\n",
    "\n",
    "#Fit model\n",
    "model_wc.fit(X_flat_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_wc=model_wc.predict(X_flat_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_wc=get_R2(y_valid,y_valid_predicted_wc)\n",
    "print('R2s:', R2s_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_evaluate(degree):\n",
    "    model_wc=WienerCascadeDecoder(degree) #Define model\n",
    "    model_wc.fit(X_flat_train,y_train) #Fit model\n",
    "    y_valid_predicted_wc=model_wc.predict(X_flat_valid) #Validation set predictions\n",
    "    return np.mean(get_R2(y_valid,y_valid_predicted_wc)) #R2 value of validation set (mean over x and y position/velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/jngvvwrj1p722fqyx4vypp540000gn/T/ipykernel_58127/839653170.py:8: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  wcBO.maximize(init_points=5, n_iter=5, kappa=10)\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "#Define Bayesian optimization, and set limits of hyperparameters \n",
    "#Here, we set the limit of \"degree\" to be [1, 6.99], so we test degrees 1,2,3,4,5,6\n",
    "wcBO = BayesianOptimization(wc_evaluate, {'degree': (1, 20.99)}, verbose=0)\n",
    "#Set number of initial runs (init_points) and subsequent tests (n_iter), and do the optimization\n",
    "#kappa is a parameter that sets exploration vs exploitation in the algorithm\n",
    "#We set kappa=10 (greater than the default) so there is more exploration when there are more hyperparameters\n",
    "wcBO.maximize(init_points=5, n_iter=5, kappa=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': -0.5582820176649397, 'params': {'degree': 19.079458960870294}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcBO.res\n",
    "wcBO.res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2s: [-5.49435565  0.5512622 ]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_wc=WienerCascadeDecoder(degree=16)\n",
    "\n",
    "#Fit model\n",
    "model_wc.fit(X_flat_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_wc=model_wc.predict(X_flat_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_wc=get_R2(y_valid,y_valid_predicted_wc)\n",
    "print('R2s:', R2s_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('results/decoding_data_1600ms_wc_optimized_pa29.mat',{'y_valid': y_valid,'y_train_mean': y_train_mean, 'y_valid_predicted_wc': y_valid_predicted_wc,'R2s_wc':R2s_wc, 'wcBO':wcBO})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:17:50] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:17:50] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:18:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:18:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "R2s: [0.53280753 0.55831052]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_xgb=XGBoostDecoder(max_depth=3,num_round=200,eta=0.3,gpu=-1) \n",
    "\n",
    "#Fit model\n",
    "model_xgb.fit(X_flat_train, y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_xgb=model_xgb.predict(X_flat_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_xgb=get_R2(y_valid,y_valid_predicted_xgb)\n",
    "print('R2s:', R2s_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_evaluate(max_depth,num_round,eta):\n",
    "    #The parameters need to be in the correct format for the decoder, so we do that below\n",
    "    max_depth=int(max_depth) \n",
    "    num_round=int(num_round) \n",
    "    eta=float(eta) \n",
    "    #Define model\n",
    "    model_xgb=XGBoostDecoder(max_depth=max_depth, num_round=num_round, eta=eta) \n",
    "    model_xgb.fit(X_flat_train,y_train) #Fit model\n",
    "    y_valid_predicted_xgb=model_xgb.predict(X_flat_valid) #Get validation set predictions\n",
    "    return np.mean(get_R2(y_valid,y_valid_predicted_xgb)) #Return mean validation set R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:18:28] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:18:28] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/jngvvwrj1p722fqyx4vypp540000gn/T/ipykernel_58127/2013187440.py:4: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  xgbBO.maximize(init_points=10, n_iter=10, kappa=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:18:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:18:44] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:19:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:19:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:19:42] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:19:42] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:20:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:20:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:21:31] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:21:31] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:22:41] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:22:41] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:23:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:23:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:23:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:23:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:23:59] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:23:59] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:24:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:24:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:25:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:25:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:26:08] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:26:08] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:27:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:27:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:27:54] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:27:54] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:28:56] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:28:56] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:30:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:30:01] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:30:56] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:30:56] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:31:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:31:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:32:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:32:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:33:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:33:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:34:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:34:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:35:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:35:51] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:36:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:36:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:36:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:36:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:38:28] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:38:28] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:40:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:40:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:41:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:41:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:42:16] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:42:16] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:42:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:42:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:42:30] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:42:30] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:43:12] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:43:12] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:43:54] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:43:54] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:44:33] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:44:33] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:45:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:45:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:45:55] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:45:55] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:46:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:46:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:47:30] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:47:30] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:48:27] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:48:27] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:48:56] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:48:56] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Do bayesian optimization, and set limits of hyperparameters\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'max_depth': (2, 6.99), 'num_round': (100,600.99), 'eta': (0.01, 0.8)},verbose=0) #Define Bayesian optimization, and set limits of hyperparameters    \n",
    "#Set number of initial runs and subsequent tests, and do the optimization. Also, we set kappa=10 (greater than the default) so there is more exploration when there are more hyperparameters\n",
    "xgbBO.maximize(init_points=10, n_iter=10, kappa=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.5537167116926374,\n",
       " 'params': {'eta': 0.26166178965809944,\n",
       "  'max_depth': 3.580469109367656,\n",
       "  'num_round': 597.9827341157404}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbBO.res\n",
    "xgbBO.res[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:49:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:49:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[00:51:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/objective/regression_obj.cu:213: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[00:51:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1667849653518/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "R2s: [0.52803671 0.55191876]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_xgb=XGBoostDecoder(max_depth=7,num_round=600,eta=0.23,gpu=-1) \n",
    "\n",
    "#Fit model\n",
    "model_xgb.fit(X_flat_train, y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_xgb=model_xgb.predict(X_flat_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_xgb=get_R2(y_valid,y_valid_predicted_xgb)\n",
    "print('R2s:', R2s_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('results/decoding_data_1600ms_xgb_optimized_pa29.mat',{'y_valid': y_valid,'y_train_mean': y_train_mean, 'y_valid_predicted_xgb': y_valid_predicted_xgb,'R2s_xgb':R2s_xgb,'xgbBO':xgbBO})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=4000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=4000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2s: [0.55959022 0.62013689]\n"
     ]
    }
   ],
   "source": [
    "#The SVR works much better when the y values are normalized, so we first z-score the y values\n",
    "#They have previously been zero-centered, so we will just divide by the stdev (of the training set)\n",
    "y_train_std=np.nanstd(y_train,axis=0)\n",
    "y_zscore_train=y_train/y_train_std\n",
    "y_zscore_test=y_test/y_train_std\n",
    "y_zscore_valid=y_valid/y_train_std\n",
    "\n",
    "#Declare model\n",
    "model_svr=SVRDecoder(C=5, max_iter=4000)\n",
    "\n",
    "#Fit model\n",
    "model_svr.fit(X_flat_train,y_zscore_train)\n",
    "\n",
    "#Get predictions\n",
    "y_zscore_valid_predicted_svr=model_svr.predict(X_flat_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_svr=get_R2(y_zscore_valid,y_zscore_valid_predicted_svr)\n",
    "print('R2s:', R2s_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('results/decoding_data_1600ms_svr_.mat',{'y_valid': y_valid,'y_train_mean': y_train_mean, 'y_zscore_valid_predicted_svr': y_zscore_valid_predicted_svr,'R2s_svr':R2s_svr}) #,'svrBO':svrBO})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_evaluate(C,max_iter):\n",
    "    #The parameters need to be in the correct format for the decoder, so we do that below\n",
    "    C=int(C) \n",
    "    max_iter=int(max_iter) \n",
    "    #Define model\n",
    "    model_svr=SVRDecoder(C=C, max_iter=max_iter) \n",
    "    model_svr.fit(X_flat_train,y_zscore_train) #Fit model\n",
    "    y_zscore_valid_predicted_svr=model_svr.predict(X_flat_valid) #Get validation set predictions\n",
    "    return np.mean(get_R2(y_zscore_valid,y_zscore_valid_predicted_svr)) #Return mean validation set R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/jngvvwrj1p722fqyx4vypp540000gn/T/ipykernel_58127/2635462230.py:4: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  svrBO.maximize(init_points=10, n_iter=10, kappa=10)\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=231).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=231).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=124).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=124).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=393).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=393).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=568).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=568).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=512).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=512).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=467).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=467).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=433).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=433).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=182).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=182).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=278).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=278).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=435).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=435).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=540).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=540).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=600).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=600).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=336).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=336).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=494).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=494).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=586).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=586).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=503).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=503).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=363).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=363).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=526).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=526).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=555).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=555).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=415).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/kendranoneman/opt/anaconda3/envs/root_env/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=415).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Do bayesian optimization, and set limits of hyperparameters\n",
    "svrBO = BayesianOptimization(svr_evaluate, {'C': (2, 6.99), 'max_iter': (100,600.99)},verbose=0) #Define Bayesian optimization, and set limits of hyperparameters    \n",
    "#Set number of initial runs and subsequent tests, and do the optimization. Also, we set kappa=10 (greater than the default) so there is more exploration when there are more hyperparameters\n",
    "svrBO.maximize(init_points=10, n_iter=10, kappa=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.320147748112657,\n",
       " 'params': {'C': 2.3629355785514825, 'max_iter': 555.2959428124}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svrBO.res\n",
    "svrBO.res[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense NN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 01:22:12.643807: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 685us/step\n",
      "R2s: [0.59909604 0.59117915]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_dnn=DenseNNDecoder(units=400,dropout=0.25,num_epochs=10)\n",
    "\n",
    "#Fit model\n",
    "model_dnn.fit(X_flat_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_dnn=model_dnn.predict(X_flat_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_dnn=get_R2(y_valid,y_valid_predicted_dnn)\n",
    "print('R2s:', R2s_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('results/decoding_data_1600ms_dnn_optimized_pa29.mat',{'y_valid': y_valid,'y_train_mean': y_train_mean, 'y_valid_predicted_dnn': y_valid_predicted_dnn,'R2s_dnn':R2s_dnn})#, 'dnnBO':dnnBO})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_evaluate(num_units,frac_dropout,n_epochs):\n",
    "    #The parameters need to be in the correct format for the decoder, so we do that below\n",
    "    num_units=int(num_units)\n",
    "    frac_dropout=float(frac_dropout)\n",
    "    n_epochs=int(n_epochs)\n",
    "    #Declare and fit decoder\n",
    "    model_dnn=DenseNNDecoder(units=[num_units,num_units],dropout=frac_dropout,num_epochs=n_epochs)\n",
    "    model_dnn.fit(X_flat_train,y_train)\n",
    "    #Make predictions and get R2 values on validation set\n",
    "    y_valid_predicted_dnn=model_dnn.predict(X_flat_valid)\n",
    "    return np.mean(get_R2(y_valid,y_valid_predicted_dnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/jngvvwrj1p722fqyx4vypp540000gn/T/ipykernel_58127/984419246.py:6: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  dnnBO.maximize(init_points=10, n_iter=10, kappa=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 1ms/step\n",
      "336/336 [==============================] - 0s 828us/step\n",
      "336/336 [==============================] - 0s 786us/step\n",
      "336/336 [==============================] - 0s 1ms/step\n",
      "336/336 [==============================] - 0s 670us/step\n",
      "336/336 [==============================] - 0s 591us/step\n",
      "336/336 [==============================] - 0s 1ms/step\n",
      "336/336 [==============================] - 0s 745us/step\n",
      "336/336 [==============================] - 0s 710us/step\n",
      "336/336 [==============================] - 0s 607us/step\n",
      "336/336 [==============================] - 0s 1ms/step\n",
      "336/336 [==============================] - 0s 611us/step\n",
      "336/336 [==============================] - 0s 906us/step\n",
      "336/336 [==============================] - 0s 984us/step\n",
      "336/336 [==============================] - 0s 756us/step\n",
      "336/336 [==============================] - 0s 1ms/step\n",
      "336/336 [==============================] - 0s 1ms/step\n",
      "336/336 [==============================] - 0s 727us/step\n",
      "336/336 [==============================] - 0s 577us/step\n",
      "336/336 [==============================] - 0s 649us/step\n"
     ]
    }
   ],
   "source": [
    "#Do bayesian optimization, and set limits of hyperparameters\n",
    "dnnBO = BayesianOptimization(dnn_evaluate, {'num_units': (50, 700.99), 'frac_dropout': (0,.5), 'n_epochs': (2,15.99)},verbose=0)\n",
    "\n",
    "#Set number of initial runs (init_points) and subsequent tests (n_iter), and do the optimization\n",
    "#kappa is a parameter that sets exploration vs exploitation in the algorithm - 10 seems to work pretty welldnnBO = BayesianOptimization(dnn_evaluate, {'num_units': (50, 500), 'frac_dropout': (0.,.5), 'n_epochs': (2,15)})\n",
    "dnnBO.maximize(init_points=10, n_iter=10, kappa=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.6619278215360211,\n",
       " 'params': {'frac_dropout': 0.3771631130961576,\n",
       "  'n_epochs': 2.531066233624182,\n",
       "  'num_units': 631.0109185361835}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnnBO.res\n",
    "dnnBO.res[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 758us/step\n",
      "R2s: [0.50496825 0.56098767]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_dnn=DenseNNDecoder(units=539,dropout=0.05,num_epochs=9)\n",
    "\n",
    "#Fit model\n",
    "model_dnn.fit(X_flat_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_dnn=model_dnn.predict(X_flat_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_dnn=get_R2(y_valid,y_valid_predicted_dnn)\n",
    "print('R2s:', R2s_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 1s 3ms/step\n",
      "R2s: [0.68088201 0.63777865]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_rnn=SimpleRNNDecoder(units=400,dropout=0,num_epochs=5)\n",
    "\n",
    "#Fit model\n",
    "model_rnn.fit(X_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_rnn=model_rnn.predict(X_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_rnn=get_R2(y_valid,y_valid_predicted_rnn)\n",
    "print('R2s:', R2s_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_evaluate(num_units,frac_dropout,n_epochs):\n",
    "    #The parameters need to be in the correct format for the decoder, so we do that below\n",
    "    num_units=int(num_units)\n",
    "    frac_dropout=float(frac_dropout)\n",
    "    n_epochs=int(n_epochs)\n",
    "    #Declare and fit decoder\n",
    "    model_rnn=SimpleRNNDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "    model_rnn.fit(X_train,y_train)\n",
    "    #Make predictions and get R2 values on validation set\n",
    "    y_valid_predicted_rnn=model_rnn.predict(X_valid)\n",
    "    return np.mean(get_R2(y_valid,y_valid_predicted_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/jngvvwrj1p722fqyx4vypp540000gn/T/ipykernel_58127/3899632513.py:6: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  rnnBO.maximize(init_points=10, n_iter=10, kappa=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 1s 4ms/step\n",
      "336/336 [==============================] - 2s 5ms/step\n",
      "336/336 [==============================] - 1s 3ms/step\n",
      "336/336 [==============================] - 1s 3ms/step\n",
      "336/336 [==============================] - 1s 3ms/step\n",
      "336/336 [==============================] - 1s 2ms/step\n",
      "336/336 [==============================] - 0s 870us/step\n",
      "336/336 [==============================] - 2s 5ms/step\n",
      "336/336 [==============================] - 1s 3ms/step\n",
      "336/336 [==============================] - 1s 2ms/step\n",
      "336/336 [==============================] - 1s 4ms/step\n",
      "336/336 [==============================] - 1s 4ms/step\n",
      "336/336 [==============================] - 2s 4ms/step\n",
      "336/336 [==============================] - 1s 4ms/step\n",
      "336/336 [==============================] - 1s 4ms/step\n",
      "336/336 [==============================] - 2s 5ms/step\n",
      "336/336 [==============================] - 1s 2ms/step\n",
      "336/336 [==============================] - 0s 954us/step\n",
      "336/336 [==============================] - 1s 2ms/step\n",
      "336/336 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "#Do bayesian optimization, and set limits of hyperparameters\n",
    "rnnBO = BayesianOptimization(rnn_evaluate, {'num_units': (50, 800.99), 'frac_dropout': (0,.5), 'n_epochs': (2,15.99)},verbose=0)\n",
    "\n",
    "#Set number of initial runs (init_points) and subsequent tests (n_iter), and do the optimization\n",
    "#kappa is a parameter that sets exploration vs exploitation in the algorithm - 10 seems to work pretty welldnnBO = BayesianOptimization(dnn_evaluate, {'num_units': (50, 500), 'frac_dropout': (0.,.5), 'n_epochs': (2,15)})\n",
    "rnnBO.maximize(init_points=10, n_iter=10, kappa=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.5663411186484788,\n",
       " 'params': {'frac_dropout': 0.40841783745335314,\n",
       "  'n_epochs': 7.889821630187229,\n",
       "  'num_units': 791.441384693681}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnnBO.res\n",
    "rnnBO.res[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 1s 2ms/step\n",
      "R2s: [0.62162645 0.60742204]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_rnn=SimpleRNNDecoder(units=352,dropout=0.08,num_epochs=4)\n",
    "\n",
    "#Fit model\n",
    "model_rnn.fit(X_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_rnn=model_rnn.predict(X_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_rnn=get_R2(y_valid,y_valid_predicted_rnn)\n",
    "print('R2s:', R2s_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('results/decoding_data_1600ms_rnn_optimized_pa29.mat',{'y_valid': y_valid,'y_train_mean': y_train_mean, 'y_valid_predicted_rnn': y_valid_predicted_rnn,'R2s_rnn':R2s_rnn,'rnnBO':rnnBO})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 2s 5ms/step\n",
      "R2s: [0.69883805 0.67471422]\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_gru=GRUDecoder(units=400,dropout=0,num_epochs=5)\n",
    "\n",
    "#Fit model\n",
    "model_gru.fit(X_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_gru=model_gru.predict(X_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_gru=get_R2(y_valid,y_valid_predicted_gru)\n",
    "print('R2s:', R2s_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_evaluate(num_units,frac_dropout,n_epochs):\n",
    "    #The parameters need to be in the correct format for the decoder, so we do that below\n",
    "    num_units=int(num_units)\n",
    "    frac_dropout=float(frac_dropout)\n",
    "    n_epochs=int(n_epochs)\n",
    "    #Declare and fit decoder\n",
    "    model_rnn=GRUDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "    model_rnn.fit(X_train,y_train)\n",
    "    #Make predictions and get R2 values on validation set\n",
    "    y_valid_predicted_rnn=model_rnn.predict(X_valid)\n",
    "    return np.mean(get_R2(y_valid,y_valid_predicted_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/jngvvwrj1p722fqyx4vypp540000gn/T/ipykernel_58127/1443782006.py:6: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  gruBO.maximize(init_points=10, n_iter=10, kappa=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 1s 4ms/step\n",
      "336/336 [==============================] - 2s 6ms/step\n",
      "336/336 [==============================] - 1s 3ms/step\n",
      "336/336 [==============================] - 4s 11ms/step\n",
      "336/336 [==============================] - 2s 7ms/step\n",
      "336/336 [==============================] - 5s 13ms/step\n",
      "336/336 [==============================] - 1s 4ms/step\n",
      "336/336 [==============================] - 3s 9ms/step\n",
      "336/336 [==============================] - 4s 10ms/step\n",
      "336/336 [==============================] - 1s 2ms/step\n",
      "336/336 [==============================] - 4s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "#Do bayesian optimization, and set limits of hyperparameters\n",
    "gruBO = BayesianOptimization(gru_evaluate, {'num_units': (50, 700.99), 'frac_dropout': (0,.5), 'n_epochs': (2,15.99)},verbose=0)\n",
    "\n",
    "#Set number of initial runs (init_points) and subsequent tests (n_iter), and do the optimization\n",
    "#kappa is a parameter that sets exploration vs exploitation in the algorithm - 10 seems to work pretty welldnnBO = BayesianOptimization(dnn_evaluate, {'num_units': (50, 500), 'frac_dropout': (0.,.5), 'n_epochs': (2,15)})\n",
    "gruBO.maximize(init_points=10, n_iter=10, kappa=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gruBO.res\n",
    "gruBO.res[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_gru=GRUDecoder(units=507,dropout=0,num_epochs=16)\n",
    "\n",
    "#Fit model\n",
    "model_gru.fit(X_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_gru=model_gru.predict(X_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_gru=get_R2(y_valid,y_valid_predicted_gru)\n",
    "print('R2s:', R2s_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('results/decoding_data_1600ms_gru_optimized_pa29.mat',{'y_valid': y_valid,'y_train_mean': y_train_mean, 'y_valid_predicted_gru': y_valid_predicted_gru,'R2s_gru':R2s_gru,'gruBO':gruBO})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from hyperopt import fmin, hp, Trials, tpe, STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_lstm=LSTMDecoder(units=400,dropout=0,num_epochs=5)\n",
    "\n",
    "#Fit model\n",
    "model_lstm.fit(X_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_lstm=model_lstm.predict(X_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_lstm=get_R2(y_valid,y_valid_predicted_lstm)\n",
    "print('R2s:', R2s_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_evaluate(num_units,frac_dropout,n_epochs):\n",
    "    #The parameters need to be in the correct format for the decoder, so we do that below\n",
    "    num_units=int(num_units)\n",
    "    frac_dropout=float(frac_dropout)\n",
    "    n_epochs=int(n_epochs)\n",
    "    #Declare and fit decoder\n",
    "    model_lstm=LSTMDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "    model_lstm.fit(X_train,y_train)\n",
    "    #Make predictions and get R2 values on validation set\n",
    "    y_valid_predicted_lstm=model_lstm.predict(X_valid)\n",
    "    return np.mean(get_R2(y_valid,y_valid_predicted_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Do bayesian optimization, and set limits of hyperparameters\n",
    "lstmBO = BayesianOptimization(lstm_evaluate, {'num_units': (50, 700.99), 'frac_dropout': (0,.5), 'n_epochs': (2,15.99)},verbose=0)\n",
    "\n",
    "#Set number of initial runs (init_points) and subsequent tests (n_iter), and do the optimization\n",
    "#kappa is a parameter that sets exploration vs exploitation in the algorithm - 10 seems to work pretty welldnnBO = BayesianOptimization(dnn_evaluate, {'num_units': (50, 500), 'frac_dropout': (0.,.5), 'n_epochs': (2,15)})\n",
    "lstmBO.maximize(init_points=10, n_iter=10, kappa=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmBO.res\n",
    "len(lstmBO.res)\n",
    "lstmBO.res[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_lstm=LSTMDecoder(units=447,dropout=0.269,num_epochs=11)\n",
    "\n",
    "#Fit model\n",
    "model_lstm.fit(X_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_lstm=model_lstm.predict(X_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_lstm=get_R2(y_valid,y_valid_predicted_lstm)\n",
    "print('R2s:', R2s_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('results/decoding_data_1600ms_lstm_optimized_pa29.mat',{'y_valid': y_valid,'y_train_mean': y_train_mean, 'y_valid_predicted_lstm': y_valid_predicted_lstm, 'R2s_lstm':R2s_lstm,'lstmBO':lstmBO})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As an example, I plot an example 1000 values of the x velocity (column index 0), both true and predicted with the Wiener filter\n",
    "#Note that I add back in the mean value, so that both true and predicted values are in the original coordinates\n",
    "fig_x_lstm=plt.figure()\n",
    "plt.plot(y_valid[1400:2400,0]+y_train_mean[0],'b')\n",
    "plt.plot(y_valid_predicted_lstm[1400:2400,0]+y_train_mean[0],'r')\n",
    "\n",
    "#Save figure\n",
    "# fig_x_wf.savefig('x_velocity_decoding.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "root_env",
   "language": "python",
   "name": "root_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
